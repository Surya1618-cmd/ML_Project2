import os
import sys
import pandas as pd
import numpy as np
import dill
from dataclasses import dataclass # Added for DataTransformationConfig

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
# Removed train_test_split as data_ingestion.py now handles it

from src.exception import CustomException
from src.logger import get_logger
from src.utils import save_object # Assuming save_object is in src/utils.py

logging = get_logger(__name__)

@dataclass
class DataTransformationConfig:
    preprocessor_file_path: str = os.path.join("artifacts", "preprocessor.pkl")
    feature_columns_file_path: str = os.path.join("artifacts", "feature_columns.pkl")
    train_processed_path: str = os.path.join("artifacts", "train_processed.csv")
    test_processed_path: str = os.path.join("artifacts", "test_processed.csv")
    
    # Paths to load raw train/test data from data_ingestion.py
    train_data_path: str = os.path.join("artifacts", "train.csv")
    test_data_path: str = os.path.join("artifacts", "test.csv")


class DataTransformation:
    def __init__(self):
        self.data_transformation_config = DataTransformationConfig()

    def get_data_transformer_object(self, numerical_features, categorical_features):
        """
        Creates and returns a ColumnTransformer for preprocessing.
        """
        numerical_transformer = StandardScaler()
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])

        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numerical_transformer, numerical_features),
                ('cat', categorical_transformer, categorical_features)
            ],
            remainder='passthrough'
        )
        return preprocessor

    def initiate_data_transformation(self, target_columns): # Removed raw_data_path
        try:
            logging.info("Starting data transformation process.")
            
            # --- Load raw train/test data from artifacts (generated by data_ingestion.py) ---
            train_df = pd.read_csv(self.data_transformation_config.train_data_path)
            test_df = pd.read_csv(self.data_transformation_config.test_data_path)
            logging.info(f"Loaded raw train_df: {train_df.shape}, test_df: {test_df.shape}")

            # --- Target Encoding (if targets are categorical 'Yes'/'No') ---
            binary_mapping = {"No": 0, "Yes": 1}
            for col in target_columns:
                if col in train_df.columns and train_df[col].dtype == 'object':
                    train_df[col] = train_df[col].map(binary_mapping)
                if col in test_df.columns and test_df[col].dtype == 'object':
                    test_df[col] = test_df[col].map(binary_mapping)
            logging.info("Binary target columns encoded.")
            # --- End Target Encoding ---

            # Fill any remaining NaNs in the raw data (e.g., from original CSV)
            train_df.fillna(method='ffill', inplace=True)
            test_df.fillna(method='ffill', inplace=True)
            logging.info("Missing values in raw data filled using ffill.")

            # Split features (X) and targets (y)
            X_train_raw = train_df.drop(columns=target_columns)
            y_train = train_df[target_columns]

            X_test_raw = test_df.drop(columns=target_columns)
            y_test = test_df[target_columns]
            logging.info(f"Data split into X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}")

            numerical_features = X_train_raw.select_dtypes(include=np.number).columns.tolist()
            categorical_features = X_train_raw.select_dtypes(include='object').columns.tolist()

            preprocessor = self.get_data_transformer_object(numerical_features, categorical_features)
            
            logging.info("Fitting preprocessor on X_train_raw...")
            preprocessor.fit(X_train_raw)

            # Save the preprocessor
            save_object(file_path=self.data_transformation_config.preprocessor_file_path, obj=preprocessor)
            logging.info(f"Preprocessor saved to {self.data_transformation_config.preprocessor_file_path}")

            # Transform data
            X_train_transformed = preprocessor.transform(X_train_raw)
            X_test_transformed = preprocessor.transform(X_test_raw)
            logging.info("Features transformed using preprocessor.")

            # Get feature names after transformation
            final_feature_columns = preprocessor.get_feature_names_out()
            
            # Convert transformed arrays back to DataFrames with correct column names
            X_train_processed = pd.DataFrame(X_train_transformed, columns=final_feature_columns)
            X_test_processed = pd.DataFrame(X_test_transformed, columns=final_feature_columns)
            
            # Save the exact feature column names
            save_object(
                file_path=self.data_transformation_config.feature_columns_file_path,
                obj=X_train_processed.columns.tolist()
            )
            logging.info(f"Feature column names saved to {self.data_transformation_config.feature_columns_file_path}: {X_train_processed.columns.tolist()}")

            # Recombine X_train_processed with y_train and save to CSVs for model_trainer.py
            train_final_df = pd.concat([X_train_processed, y_train.reset_index(drop=True)], axis=1)
            test_final_df = pd.concat([X_test_processed, y_test.reset_index(drop=True)], axis=1)

            train_final_df.to_csv(self.data_transformation_config.train_processed_path, index=False, header=True)
            test_final_df.to_csv(self.data_transformation_config.test_processed_path, index=False, header=True)
            logging.info(f"Processed train/test data saved to {self.data_transformation_config.train_processed_path} and {self.data_transformation_config.test_processed_path}")

            return (
                self.data_transformation_config.train_processed_path,
                self.data_transformation_config.test_processed_path,
                self.data_transformation_config.preprocessor_file_path,
                self.data_transformation_config.feature_columns_file_path
            )

        except Exception as e:
            logging.error(f"Error in initiate_data_transformation function: {e}", exc_info=True)
            raise CustomException(e, sys)

if __name__ == "__main__":
    target_columns = ["HeartDisease", "Diabetes", "Hypertension", "Asthma", 
                      "KidneyDisease", "LiverDisease", "Cancer", "Obesity", 
                      "Arthritis", "COPD", "MentalHealthIssue"]
    
    data_transformer = DataTransformation()
    train_processed_path, test_processed_path, preprocessor_path, feature_columns_path = data_transformer.initiate_data_transformation(
        target_columns=target_columns
    )
    logging.info(f"Data transformation complete. Processed train path: {train_processed_path}")
